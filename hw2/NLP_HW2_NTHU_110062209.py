# -*- coding: utf-8 -*-
"""NLP_HW2_NTHU_110062209.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WNq-2IvaeeelinAT1Z5Nh32eghVuLP5K
"""

from google.colab import drive
drive.mount('/content/drive')

dir = '/content/drive/MyDrive/'

import pandas as pd
import numpy as np
import torch
import torch.nn
import torch.nn.utils.rnn
import torch.utils.data
from tqdm import tqdm
torch.cuda.manual_seed_all(48758)
torch.manual_seed(48758)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

df_train = pd.read_csv(dir+'arithmetic_NLP/arithmetic_train.csv')
df_eval = pd.read_csv(dir+'arithmetic_NLP/arithmetic_eval.csv')
df_train.head()

df_train['tgt'] = df_train['tgt'].apply(lambda x:str(x))
df_train['src'] = df_train['src'].add(df_train['tgt'])

df_eval['tgt'] = df_eval['tgt'].apply(lambda x:str(x))

#TODO1:build dictionary
char_to_id = {
    '<pad>':0,
    '1':1,
    '2':2,
    '3':3,
    '4':4,
    '5':5,
    '6':6,
    '7':7,
    '8':8,
    '9':9,
    '0':10,
    '<eos>':11,
    '+':12,
    '-':13,
    '*':14,
    '(':15,
    ')':16,
    '=':17,
    '<bos>':18
}
id_to_char = {
    0:'<pad>',
    1:'1',
    2:'2',
    3:'3',
    4:'4',
    5:'5',
    6:'6',
    7:'7',
    8:'8',
    9:'9',
    10:'0',
    11:'<eos>',
    12:'+',
    13:'-',
    14:'*',
    15:'(',
    16:')',
    17:'=',
    18:'<bos>'
}

vocab_size = len(char_to_id)
print(f'vocab_size:{vocab_size}')

#TODO2:data preprocessing
def get_label(char_id_list):
  ret = []
  flag = False
  for id in char_id_list:
    if id == char_to_id['<bos>']:
      continue
    if id == char_to_id['=']:
      flag = True
      ret.append(char_to_id['<pad>'])
      continue
    if flag:
      ret.append(id)
    else:
      ret.append(char_to_id['<pad>'])
  return ret

df_train['char_id_list'] = df_train['src'].apply(lambda x:[char_to_id['<bos>']]+[char_to_id[ch] for ch in x])#<bos>a+b=c
df_train['label_id_list'] = df_train['char_id_list'].apply(lambda x:get_label(x)+[char_to_id['<eos>']])#<pad>*n c<eos>

df_eval['char_id_list'] = df_eval['src'].apply(lambda x:[char_to_id['<bos>']]+[char_to_id[ch] for ch in x])#<bos>a+b=
df_train.head()

filter_train = df_train[~df_train['src'].str.contains('9')].reset_index(drop=True)

#TODO3: Data Batching
class Dataset(torch.utils.data.Dataset):
  def __init__(self,sequences):
    self.sequences = sequences
  def __len__(self):
    return len(self.sequences)
  def __getitem__(self, index):
    x=self.sequences['char_id_list'][index]
    y=self.sequences['label_id_list'][index]
    return x,y

class CharRNN(torch.nn.Module):
  def __init__(self,vocab_size,embed_dim,hidden_dim,Type='LSTM'):
    super(CharRNN,self).__init__()
    self.hidden_dim = hidden_dim
    self.Type = Type
    self.embedding=torch.nn.Embedding(num_embeddings=vocab_size,
                      embedding_dim=embed_dim,
                      padding_idx=char_to_id['<pad>'])
    if Type=='LSTM':
      self.rnn_layer1=torch.nn.LSTM(input_size=embed_dim,
                      hidden_size=hidden_dim,
                      batch_first=True)
      self.rnn_layer2=torch.nn.LSTM(input_size=hidden_dim,
                      hidden_size=hidden_dim,
                      batch_first=True)
    elif Type=='RNN':
      self.rnn_layer1=torch.nn.RNN(input_size=embed_dim,
                      hidden_size=hidden_dim,
                      batch_first=True)
      self.rnn_layer2=torch.nn.RNN(input_size=hidden_dim,
                      hidden_size=hidden_dim,
                      batch_first=True)
    else:#Type==GRU
      self.rnn_layer1=torch.nn.GRU(input_size=embed_dim,
                      hidden_size=hidden_dim,
                      batch_first=True)
      self.rnn_layer2=torch.nn.GRU(input_size=hidden_dim,
                      hidden_size=hidden_dim,
                      batch_first=True)
    self.linear_layer=torch.nn.Sequential(torch.nn.Linear(in_features=hidden_dim,
                              out_features=hidden_dim),
                        torch.nn.ReLU(),
                        torch.nn.Linear(in_features=hidden_dim,
                              out_features=vocab_size))

  def forward(self,x,hidden1=None,hidden2=None):
    ret = self.embedding(x)
    if self.Type=='LSTM':
      if hidden1 is None:
        hidden1 = (torch.zeros(1,x.shape[0],self.hidden_dim).to(x.device),torch.zeros(1,x.shape[0],self.hidden_dim).to(x.device))
      if hidden2 is None:
        hidden2 = (torch.zeros(1,x.shape[0],self.hidden_dim).to(x.device),torch.zeros(1,x.shape[0],self.hidden_dim).to(x.device))
    else:
      if hidden1 is None:
        hidden1 = torch.zeros(1,x.shape[0],self.hidden_dim).to(x.device)
      if hidden2 is None:
        hidden2 = torch.zeros(1,x.shape[0],self.hidden_dim).to(x.device)

    ret,hidden1 = self.rnn_layer1(ret,hidden1)
    ret,hidden2 = self.rnn_layer2(ret,hidden2)
    ret = self.linear_layer(ret)
    return ret, hidden1, hidden2

  #TODO4: Generation
  def generator(self,start_char,max_len=200,device='cpu'):
    char_list = [c for c in start_char]
    next_char = None
    if self.Type=='LSTM':
      hidden1 = (torch.zeros(1,1,self.hidden_dim).to(device),torch.zeros(1,1,self.hidden_dim).to(device))
      hidden2 = (torch.zeros(1,1,self.hidden_dim).to(device),torch.zeros(1,1,self.hidden_dim).to(device))
    else:
      hidden1 = torch.zeros(1,1,self.hidden_dim).to(device)
      hidden2 = torch.zeros(1,1,self.hidden_dim).to(device)
    start = False
    with torch.no_grad():
      while len(char_list) < max_len:
        if not start:
          y = torch.tensor([char_list]).to(device)
          start = True
        else:
          y = torch.tensor([[char_list[-1]]]).to(device)
        y,hidden1,hidden2 = self.forward(y,hidden1,hidden2)
        next_char = torch.argmax(y[:,-1,:],dim=-1)
        if next_char == char_to_id['<eos>']:
          break
        char_list.append(next_char.detach().cpu().item())
    return [id_to_char[ch_id] for ch_id in char_list]

#setting
def collate_fn(batch):
  maxlen = max(len(seq[0]) for seq in batch)
  x = [list(seq[0]) + [char_to_id['<pad>']]*(maxlen-len(seq[0])) for seq in batch]
  y = [list(seq[1]) + [char_to_id['<pad>']]*(maxlen-len(seq[1])) for seq in batch]
  x = torch.stack([torch.tensor(seq) for seq in x])
  y = torch.stack([torch.tensor(seq) for seq in y])
  return x,y

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f'device:{device}')
epochs = 10
learning_rate = 5e-3
embed_dim = 100
hidden_dim = 200
batch_size = 5000
Type = 'LSTM'


model = CharRNN(vocab_size,embed_dim=embed_dim,hidden_dim=hidden_dim,Type=Type).to(device)
loss_func = torch.nn.CrossEntropyLoss(ignore_index=char_to_id['<pad>'])
optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)
dataset_train = Dataset(df_train)
total_num = len(dataset_train)
dataloader_train = torch.utils.data.DataLoader(dataset_train,batch_size=batch_size,collate_fn=collate_fn,shuffle=True)

#TODO5:train
model.train()
for epoch in range(epochs):
  for x,y in tqdm(dataloader_train):
    x,y = x.to(device),y.to(device)
    output,_,_ = model(x)
    loss = loss_func(output.view(-1,vocab_size),y.view(-1))
    optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)
    optimizer.step()

#TODO6:eval
def get_ans(s):
  ans = ''
  flag = False
  for ch in s:
    if ch == '=' and not flag:
      flag = True
      continue
    if flag:
      if ch == '<eos>':
        break
      ans+=ch
  return ans

model.eval()
data_num = len(df_eval['char_id_list'])
true_num = 0

for id,input_data in enumerate(df_eval['char_id_list']):
  result = get_ans(model.generator(input_data,device=device,max_len=25))
  if result == df_eval['tgt'][id]:
    true_num+=1
print(f'accuracy:{true_num/data_num}')

def ques_to_id(x):
  return [char_to_id['<bos>']]+[char_to_id[ch] for ch in x]

Q_arr = ['100+375=',
         '(300-78)*2=',
         '498-486+9=',
         '(57-759)-67=',
         '0*(758-9)=',
         '467*2-89=',
         "526-(67*11)=",
         "301*(687-686)=",
         "90-349=",
         "-974+(758-8)=",
         "100*0-896=",
         "67-987+89=",
         "83*12-900="]

for input_data in Q_arr:
  print(''.join(model.generator(ques_to_id(input_data),device=device)))