{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":2465512,"sourceType":"datasetVersion","datasetId":1492415},{"sourceId":3293072,"sourceType":"datasetVersion","datasetId":1993148},{"sourceId":7017419,"sourceType":"datasetVersion","datasetId":3937250},{"sourceId":8358085,"sourceType":"datasetVersion","datasetId":4966859},{"sourceId":8404441,"sourceType":"datasetVersion","datasetId":4956698},{"sourceId":8404458,"sourceType":"datasetVersion","datasetId":5001044},{"sourceId":8859067,"sourceType":"datasetVersion","datasetId":4832208},{"sourceId":10032007,"sourceType":"datasetVersion","datasetId":6178752},{"sourceId":10088876,"sourceType":"datasetVersion","datasetId":6220795}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --no-index --no-deps /kaggle/input/aes-whls/aes_whls/huggingface_hub-0.23.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/aes-whls/aes_whls/safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install --no-index --no-deps /kaggle/input/aes-whls/aes_whls/sentence_transformers-2.8.0.dev0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/aes-whls/aes_whls/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install --no-index --no-deps /kaggle/input/aes-whls/aes_whls/transformers-4.40.2-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/aes-whls/aes_whls/textstat-0.7.3-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/aes-whls/aes_whls/pyphen-0.15.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/aes-whls/aes_whls/einops-0.8.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/aes-whls/aes_whls/pyspellchecker-0.8.1-py3-none-any.whl","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T10:37:28.270598Z","iopub.execute_input":"2024-12-03T10:37:28.271057Z","iopub.status.idle":"2024-12-03T10:37:55.926807Z","shell.execute_reply.started":"2024-12-03T10:37:28.271004Z","shell.execute_reply":"2024-12-03T10:37:55.925478Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport polars as pl\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tokenizers import (\n    decoders,\n    models,\n    pre_tokenizers,\n    normalizers,\n    processors,\n    trainers,\n    Tokenizer\n)\n\nfrom datasets import Dataset\nfrom tqdm.auto import tqdm\nfrom transformers import PreTrainedTokenizerFast\n\nimport gc\n\nimport spacy\nfrom collections import Counter\n\nimport nltk\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport textstat\nfrom spellchecker import SpellChecker\n\nfrom sentence_transformers import SentenceTransformer, models\nfrom sklearn.linear_model import Ridge\nfrom sklearn.multioutput import MultiOutputRegressor\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import cohen_kappa_score\nimport torch\n\nimport copy\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments,DataCollatorWithPadding\nfrom datasets import Dataset\nfrom glob import glob\nimport re\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\nimport gc\nimport lightgbm as lgb\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nimport re\nimport random\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier,GradientBoostingClassifier,BaggingClassifier\nfrom sklearn.linear_model import LogisticRegression, Perceptron\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB,ComplementNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom lightgbm import log_evaluation, early_stopping\nfrom sklearn.linear_model import SGDClassifier\nimport pickle\nfrom scipy.special import softmax\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport transformers as T\nimport torch\nfrom tqdm import tqdm\nfrom sklearn.cluster import KMeans\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nnltk.download('wordnet')\nnltk.download('punkt_tab')\nnltk.download('vader_lexicon')\n\ntqdm.pandas()\n\nnlp = spacy.load(\"en_core_web_sm\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-03T10:37:55.929400Z","iopub.execute_input":"2024-12-03T10:37:55.930208Z","iopub.status.idle":"2024-12-03T10:39:55.917566Z","shell.execute_reply.started":"2024-12-03T10:37:55.930160Z","shell.execute_reply":"2024-12-03T10:39:55.916376Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Load data**","metadata":{}},{"cell_type":"code","source":"competition_dir = '/kaggle/input/learning-agency-lab-automated-essay-scoring-2'\npersaude_dir = 'kaggle/input/persaude-corpus-2'\ntrain = pd.read_csv(competition_dir+'/train.csv')\ntest = pd.read_csv(competition_dir+'/test.csv')\npersuade = pd.read_csv('/kaggle/input/persaude-corpus-2/persuade_2.0_human_scores_demo_id_github.csv')\nsample_submission = pd.read_csv(competition_dir+'/sample_submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T10:39:55.919079Z","iopub.execute_input":"2024-12-03T10:39:55.920108Z","iopub.status.idle":"2024-12-03T10:39:58.187748Z","shell.execute_reply.started":"2024-12-03T10:39:55.920055Z","shell.execute_reply":"2024-12-03T10:39:58.187023Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **textstat**","metadata":{}},{"cell_type":"code","source":"def textstat_features(text):\n    features = {}\n    features['flesch_reading_ease'] = textstat.flesch_reading_ease(text)\n    features['flesch_kincaid_grade'] = textstat.flesch_kincaid_grade(text)\n    features['smog_index'] = textstat.smog_index(text)\n    features['coleman_liau_index'] = textstat.coleman_liau_index(text)\n    features['automated_readability_index'] = textstat.automated_readability_index(text)\n    features['dale_chall_readability_score'] = textstat.dale_chall_readability_score(text)\n    features['difficult_words'] = textstat.difficult_words(text)\n    features['linsear_write_formula'] = textstat.linsear_write_formula(text)\n    features['gunning_fog'] = textstat.gunning_fog(text)\n    features['text_standard'] = textstat.text_standard(text, float_output=True)\n    features['spache_readability'] = textstat.spache_readability(text)\n    features['mcalpine_eflaw'] = textstat.mcalpine_eflaw(text)\n    features['reading_time'] = textstat.reading_time(text)\n    features['syllable_count'] = textstat.syllable_count(text)\n    features['lexicon_count'] = textstat.lexicon_count(text)\n    features['monosyllabcount'] = textstat.monosyllabcount(text)\n\n    return features\n\ntrain['textstat_features'] = train['full_text'].apply(textstat_features)\ntrain_textstat = pd.DataFrame(train['textstat_features'].tolist())\n\ntest['textstat_features'] = test['full_text'].apply(textstat_features)\ntest_textstat = pd.DataFrame(test['textstat_features'].tolist())\n\ntrain_textstat.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **linguistic**","metadata":{}},{"cell_type":"code","source":"def extract_linguistic_features(text):\n\n    doc = nlp(text)\n    features = {}\n\n    # NER Features\n    entity_counts = {\"GPE\": 0, \"PERCENT\": 0, \"NORP\": 0, \"ORG\": 0, \"CARDINAL\": 0, \"MONEY\": 0, \"DATE\": 0,\n                    \"LOC\": 0, \"PERSON\": 0, \"QUANTITY\": 0, \"EVENT\": 0, \"ORDINAL\": 0, \"WORK_OF_ART\": 0,\n                    \"LAW\": 0, \"PRODUCT\": 0, \"TIME\": 0, \"FAC\": 0, \"LANGUAGE\": 0}\n    for entity in doc.ents:\n        if entity.label_ in entity_counts:\n            entity_counts[entity.label_] += 1\n    features['NER_Features'] = entity_counts\n\n    # POS Features\n    pos_counts = {\"ADJ\": 0, \"NOUN\": 0, \"VERB\": 0, \"SCONJ\": 0, \"PRON\": 0, \"PUNCT\": 0, \"DET\": 0, \"AUX\": 0,\n                \"PART\": 0, \"ADP\": 0, \"SPACE\": 0, \"CCONJ\": 0, \"PROPN\": 0, \"NUM\": 0, \"ADV\": 0,\n                \"SYM\": 0, \"INTJ\": 0, \"X\": 0}\n    for token in doc:\n        if token.pos_ in pos_counts:\n            pos_counts[token.pos_] += 1\n    features['POS_Features'] = pos_counts\n\n    # tag Features\n    tags = {\"RB\": 0, \"-RRB-\": 0, \"PRP$\": 0, \"JJ\": 0, \"TO\": 0, \"VBP\": 0, \"JJS\": 0, \"DT\": 0, \"''\": 0, \"UH\": 0, \"RBS\": 0, \"WRB\": 0, \".\": 0,\n        \"HYPH\": 0, \"XX\": 0, \"``\": 0, \"SYM\": 0, \"VB\": 0, \"VBN\": 0, \"WP\": 0, \"CC\": 0, \"LS\": 0, \"POS\": 0, \"NN\": 0, \",\": 0, \"NNPS\": 0,\n          \"RP\": 0, \":\": 0, \"$\": 0, \"PDT\": 0, \"VBZ\": 0, \"VBD\": 0, \"JJR\": 0, \"-LRB-\": 0, \"IN\": 0, \"RBR\": 0, \"WDT\": 0, \"EX\": 0, \"MD\": 0,\n            \"_SP\": 0, \"NNP\": 0, \"CD\": 0, \"VBG\": 0, \"NNS\": 0, \"PRP\": 0}\n\n    for token in doc:\n        if token.tag_ in tags:\n            tags[token.tag_] += 1\n    features['tag_Features'] = tags\n\n    # tense features\n    tenses = [i.morph.get(\"Tense\") for i in doc]\n    tenses = [i[0] for i in tenses if i]\n    tense_counts = Counter(tenses)\n    features['past_tense_ratio'] = tense_counts.get(\"Past\", 0) / (tense_counts.get(\"Pres\", 0) + tense_counts.get(\"Past\", 0) + 1e-5)\n    features['present_tense_ratio'] = tense_counts.get(\"Pres\", 0) / (tense_counts.get(\"Pres\", 0) + tense_counts.get(\"Past\", 0) + 1e-5)\n\n\n    # len features\n\n    features['word_count'] = len(doc)\n    features['sentence_count'] = len([sentence for sentence in doc.sents])\n    features['words_per_sentence'] = features['word_count'] / features['sentence_count']\n    features['std_words_per_sentence'] = np.std([len(sentence) for sentence in doc.sents])\n\n    features['unique_words'] = len(set([token.text for token in doc]))\n    features['lexical_diversity'] = features['unique_words'] / features['word_count']\n\n    paragraph = text.split('\\n\\n')\n\n    features['paragraph_count'] = len(paragraph)\n\n    features['avg_chars_by_paragraph'] = np.mean([len(paragraph) for paragraph in paragraph])\n    features['avg_words_by_paragraph'] = np.mean([len(nltk.word_tokenize(paragraph)) for paragraph in paragraph])\n    features['avg_sentences_by_paragraph'] = np.mean([len(nltk.sent_tokenize(paragraph)) for paragraph in paragraph])\n\n    # sentiment features\n    analyzer = SentimentIntensityAnalyzer()\n    sentences = nltk.sent_tokenize(text)\n\n    compound_scores, negative_scores, positive_scores, neutral_scores = [], [], [], []\n    for sentence in sentences:\n        scores = analyzer.polarity_scores(sentence)\n        compound_scores.append(scores['compound'])\n        negative_scores.append(scores['neg'])\n        positive_scores.append(scores['pos'])\n        neutral_scores.append(scores['neu'])\n\n    features[\"mean_compound\"] = np.mean(compound_scores)\n    features[\"mean_negative\"] = np.mean(negative_scores)\n    features[\"mean_positive\"] = np.mean(positive_scores)\n    features[\"mean_neutral\"] = np.mean(neutral_scores)\n\n    features[\"std_compound\"] = np.std(compound_scores)\n    features[\"std_negative\"] = np.std(negative_scores)\n    features[\"std_positive\"] = np.std(positive_scores)\n    features[\"std_neutral\"] = np.std(neutral_scores)\n\n    return features\n\ntrain['linguistic_features'] = train['full_text'].progress_apply(extract_linguistic_features)\n\ntrain_linguistic = pd.json_normalize(train['linguistic_features'])\n\n\n\ntest['linguistic_features'] = test['full_text'].progress_apply(extract_linguistic_features)\n\ntest_linguistic = pd.json_normalize(test['linguistic_features'])\n\ntrain_linguistic.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tag_cols = [col for col in train_linguistic.columns if col.startswith('tag')]\ncol_cols = [col for col in train_linguistic.columns if col.startswith('col')]\npos_cols = [col for col in train_linguistic.columns if col.startswith('pos')]\n\nfor col in tag_cols:\n    train_linguistic[f\"{col}_ratio\"] = train_linguistic[col] / train_linguistic['word_count']\n    test_linguistic[f\"{col}_ratio\"] = test_linguistic[col] / test_linguistic['word_count']\n\nfor col in col_cols:\n    test_linguistic[f\"{col}_ratio\"] = test_linguistic[col] / test_linguistic['word_count']\n\nfor col in pos_cols:\n    test_linguistic[f\"{col}_ratio\"] = test_linguistic[col] / test_linguistic['word_count']\n\ntrain_linguistic.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df = pd.concat([train_textstat, train_linguistic], axis=1)\n\nmerged_df_test = pd.concat([test_textstat, test_linguistic], axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **SpellCheck**","metadata":{}},{"cell_type":"code","source":"spell = SpellChecker()\n\ndef spell_check(text):\n    words = nltk.word_tokenize(text)\n    misspelled = spell.unknown(words)\n\n    mispelled_count = len(misspelled)\n    misspelled_ratio = mispelled_count / len(words)\n\n    return mispelled_count, misspelled_ratio\n\ntrain['spell_check_features'] = train['full_text'].progress_apply(spell_check)\n\nspell_check_df = pd.DataFrame(train['spell_check_features'].tolist(), columns=['misspelled_count', 'misspelled_ratio'])\n\ntest['spell_check_features'] = test['full_text'].progress_apply(spell_check)\n\ntest_check_df = pd.DataFrame(test['spell_check_features'].tolist(), columns=['misspelled_count', 'misspelled_ratio'])\n\nspell_check_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df = pd.concat((merged_df, spell_check_df), axis=1)\n\nmerged_df_test = pd.concat((merged_df_test, test_check_df), axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **feedback**","metadata":{}},{"cell_type":"code","source":"feedback_path = '/kaggle/input/feedback-data'\nsentpath = '/kaggle/input/sent-debsmall'\nfeedback_df = pd.read_csv(feedback_path+'/feedback_data.csv')\n\nfeed_embeds = []\n\nmerged_embeds = []\n\ntest_embeds = []\n\nfor i in range(5):\n    model_path = sentpath + f'/deberta_small_trained/temp_fold{i}_checkpoints'\n    word_embedding_model = models.Transformer(model_path, max_seq_length=1024)\n    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n    model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n\n    model.half()\n    model = model.to(device)\n\n    feed_custom_embeddings_train = model.encode(feedback_df.loc[:, 'full_text'].values, device=device,\n                                                show_progress_bar=True, normalize_embeddings=True)\n\n    feed_embeds.append(feed_custom_embeddings_train)\n\n    merged_custom_embeddings = model.encode(train.loc[:, 'full_text'].values, device=device,\n                                            show_progress_bar=True, normalize_embeddings=True)\n\n    merged_embeds.append(merged_custom_embeddings)\n\n\n    test_custom_embeddings = model.encode(test.loc[:, 'full_text'].values, device=device,\n                                            show_progress_bar=True, normalize_embeddings=True)\n\n    test_embeds.append(test_custom_embeddings)\n\nfeed_embeds = np.mean(feed_embeds, axis=0)\nmerged_embeds = np.mean(merged_embeds, axis=0)\ntest_embeds = np.mean(test_embeds, axis=0)\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"targets = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n\n\nridge = Ridge(alpha=1.0)\n\nmultioutputregressor = MultiOutputRegressor(ridge)\n\n\n\nmultioutputregressor.fit(feed_embeds, feedback_df.loc[:, targets])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"feedback_predictions = multioutputregressor.predict(merged_embeds)\n\nfeedback_predictions_df = pd.DataFrame(feedback_predictions, columns=targets)\n\ntest_feedback_predictions = multioutputregressor.predict(test_embeds)\n\ntest_feedback_predictions_df = pd.DataFrame(test_feedback_predictions, columns=targets)\n\nfeedback_predictions_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df = pd.concat((merged_df, feedback_predictions_df), axis=1)\n\nmerged_df_test = pd.concat((merged_df_test, test_feedback_predictions_df), axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **topic**","metadata":{}},{"cell_type":"code","source":"tokenizer = T.BertTokenizer.from_pretrained(\"/kaggle/input/bert-base-uncased\", cache_dir=\"./cache/\")\nmodel = T.AutoModel.from_pretrained(\"/kaggle/input/bert-base-uncased\").to(device)\n# from transformers import AutoTokenizer, AutoModel\n# tokenizer = AutoTokenizer.from_pretrained(\n#     \"/kaggle/input/deberta-v3-large/deberta-v3-large\",\n#     cache_dir=\"./cache/\"\n# )\n# model = AutoModel.from_pretrained(\n#     \"/kaggle/input/deberta-v3-large/deberta-v3-large\"\n# ).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T10:46:18.379224Z","iopub.execute_input":"2024-12-03T10:46:18.379593Z","iopub.status.idle":"2024-12-03T10:46:20.831958Z","shell.execute_reply.started":"2024-12-03T10:46:18.379559Z","shell.execute_reply":"2024-12-03T10:46:20.831188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def text_to_bert_vector(text):\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)['input_ids'].to(device)\n    with torch.no_grad():\n        outputs = model(inputs)\n    cls_vector = outputs.last_hidden_state[:, 0, :]\n    return cls_vector.squeeze().detach().cpu().numpy()\n\ndef transfer_bert(full_text_list,batch_size = 100):\n    bert_vectors = []\n    for i in tqdm(range(0, len(full_text_list), batch_size)):\n        batch_text = full_text_list[i:i + batch_size]\n        try:\n            bert_vector_batch = text_to_bert_vector(batch_text)\n            bert_vectors.append(bert_vector_batch)\n        except Exception as e:\n            batch_text = full_text_list[i:]\n            bert_vector_batch = text_to_bert_vector(batch_text)\n            bert_vectors.append(bert_vector_batch)\n    bert_vectors = [vec for vec in bert_vectors if vec is not None]\n    bert_vectors_array = np.vstack(bert_vectors)\n    return bert_vectors_array","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T10:43:58.904509Z","iopub.execute_input":"2024-12-03T10:43:58.905211Z","iopub.status.idle":"2024-12-03T10:43:58.911651Z","shell.execute_reply.started":"2024-12-03T10:43:58.905172Z","shell.execute_reply":"2024-12-03T10:43:58.910813Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bert_path = '/kaggle/input/bert-cls'\npersuade_bert = np.load(bert_path + '/persuade_bert.npy')\ntrain_bert = np.load(bert_path + '/train_bert.npy')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# bert_path = '/kaggle/input/deberta'\n# persuade_bert = np.load(bert_path + '/persuade_deberta.npy')\n# train_bert = np.load(bert_path + '/train_deberta.npy')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#persuade_bert = transfer_bert(persuade['full_text'].to_list())\n#train_bert = transfer_bert(train['full_text'].to_list())\ntest_bert = transfer_bert(test['full_text'].to_list(),batch_size=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T10:45:14.924708Z","iopub.execute_input":"2024-12-03T10:45:14.925158Z","iopub.status.idle":"2024-12-03T10:45:15.436334Z","shell.execute_reply.started":"2024-12-03T10:45:14.925126Z","shell.execute_reply":"2024-12-03T10:45:15.435430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #kmeans\n# n_clusters = 15\n# kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n# kmeans.fit(persuade_bert)\n# train_topic = kmeans.predict(train_bert)\n# test_topic = kmeans.predict(test_bert)\n\n# train_topic_one_hot = np.eye(n_clusters)[train_topic]\n# test_topic_one_hot = np.eye(n_clusters)[test_topic]\n\n# topic_col = [f'topic_{i}' for i in range(15)]\n# train_topic_df = pd.DataFrame(train_topic_one_hot, columns=topic_col)\n# test_topic_df = pd.DataFrame(test_topic_one_hot, columns=topic_col)\n# merged_df = pd.concat((merged_df, train_topic_df), axis=1)\n# merged_df_test = pd.concat((merged_df_test, test_topic_df), axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#knn\nknn_clf = KNeighborsClassifier(n_neighbors=15)\nknn_clf.fit(persuade_bert, persuade['prompt_name'])\n\nencoder = OneHotEncoder(sparse=False)\nencoded_data = encoder.fit(persuade['prompt_name'].values.reshape(-1, 1))\n\ntrain_topic = knn_clf.predict(train_bert)\ntest_topic = knn_clf.predict(test_bert)\n\ntrain_topic_one_hot = encoder.transform(train_topic.reshape(-1, 1))\ntest_topic_one_hot = encoder.transform(test_topic.reshape(-1, 1))\n\ntopic_col = encoder.get_feature_names_out(input_features=['Topic'])\n\ntrain_topic_df = pd.DataFrame(train_topic_one_hot, columns=topic_col)\ntest_topic_df = pd.DataFrame(test_topic_one_hot, columns=topic_col)\n\nmerged_df = pd.concat((merged_df, train_topic_df), axis=1)\nmerged_df_test = pd.concat((merged_df_test, test_topic_df), axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Paragraph feature**","metadata":{}},{"cell_type":"code","source":"columns = [\n    (\n        pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\")\n    ),\n]\n\n# 载入训练集和测试集，同时对full_text数据使用\\n\\n字符分割为列表，重命名为paragraph\n# Load training and testing sets, while using \\ n \\ n character segmentation to list and renaming to paragraph for full_text data\ntrain_df = pl.read_csv(competition_dir+\"/train.csv\").with_columns(columns)\ntest_df = pl.read_csv(competition_dir+\"/test.csv\").with_columns(columns)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cList = {\n  \"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",  \"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\n  \"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he'll've\": \"he will have\",\"he's\": \"he is\",\n  \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\"I'd\": \"I would\",\"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\n  \"isn't\": \"is not\",\"it'd\": \"it had\",\"it'd've\": \"it would have\",\"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\n  \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n  \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n  \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"so's\": \"so is\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there had\",\"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we had\",\n  \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n  \"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\n  \"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\n  \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\n  \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\"wouldn't\": \"would not\",\n  \"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\n  \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\"you're\": \"you are\",  \"you've\": \"you have\"\n   }\n\nc_re = re.compile('(%s)' % '|'.join(cList.keys()))\n\ndef expandContractions(text, c_re=c_re):\n    def replace(match):\n        return cList[match.group(0)]\n    return c_re.sub(replace, text)\n\ndef removeHTML(x):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',x)\ndef dataPreprocessing(x):\n    x = x.lower()\n    x = removeHTML(x)\n    x = re.sub(\"@\\w+\", '',x)\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    x = re.sub(\"http\\w+\", '',x)\n    x = re.sub(r\"\\s+\", \" \", x)\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    x = x.strip()\n    return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 段落特征\n# paragraph features\ndef Paragraph_Preprocess(tmp):\n    # 将段落列表扩展为一行行的数据\n    # Expand the paragraph list into several lines of data\n    tmp = tmp.explode('paragraph')\n    # 段落预处理\n    # Paragraph preprocessing\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(dataPreprocessing))\n    # 计算每一个段落的长度\n    # Calculate the length of each paragraph\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x)).alias(\"paragraph_len\"))\n    # 计算每一个段落中句子的数量和单词的数量\n    # Calculate the number of sentences and words in each paragraph\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x.split('.'))).alias(\"paragraph_sentence_cnt\"),\n                    pl.col('paragraph').map_elements(lambda x: len(x.split(' '))).alias(\"paragraph_word_cnt\"),)\n    return tmp\n# feature_eng\nparagraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\ndef Paragraph_Eng(train_tmp):\n    aggs = [\n        # 统计段落长度大于和小于 i 值的个数\n        # Count the number of paragraph lengths greater than and less than the i-value\n        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_{i}_cnt\") for i in [50,75,100,125,150,175,200,250,300,350,400,500,600,700] ],\n        *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f\"paragraph_{i}_cnt\") for i in [25,49]],\n        # 其他\n        # other\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in paragraph_fea],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in paragraph_fea],\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\ntmp = Paragraph_Preprocess(train_df)\ntrain_feats = Paragraph_Eng(tmp)\ntrain_feats['score'] = train_df['score']\n# 获取特征名称\n# Obtain feature names\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\n\ntmp = Paragraph_Preprocess(test_df)\ntest_feats = Paragraph_Eng(tmp)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Sentence feature**","metadata":{}},{"cell_type":"code","source":"# sentence feature\ndef Sentence_Preprocess(tmp):\n    # 对full_text预处理，并且使用句号分割出文本的句子\n    # Preprocess full_text and use periods to segment sentences in the text\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\".\").alias(\"sentence\"))\n    tmp = tmp.explode('sentence')\n    # 计算句子的长度\n    # Calculate the length of a sentence\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x)).alias(\"sentence_len\"))\n    # 筛选出句子长度大于15的那一部分数据\n    # Filter out the portion of data with a sentence length greater than 15\n    tmp = tmp.filter(pl.col('sentence_len')>=15)\n    # 统计每一句中单词的数量\n    # Count the number of words in each sentence\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x.split(' '))).alias(\"sentence_word_cnt\"))\n\n    return tmp\n# feature_eng\nsentence_fea = ['sentence_len','sentence_word_cnt']\ndef Sentence_Eng(train_tmp):\n    aggs = [\n        # 统计句子长度大于 i 的句子个数\n        # Count the number of sentences with a length greater than i\n        *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_{i}_cnt\") for i in [15,50,100,150,200,250,300] ],\n        # 其他\n        # other\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in sentence_fea],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in sentence_fea],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in sentence_fea],\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ntmp = Sentence_Preprocess(train_df)\n# 将新生成的特征数据和之前生成的特征数据合并\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\n\ntmp = Sentence_Preprocess(test_df)\ntest_feats = test_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Word feature**","metadata":{}},{"cell_type":"code","source":"# word feature\ndef Word_Preprocess(tmp):\n    # 对full_text预处理，并且使用空格符分割出文本的单词\n    # Preprocess full_text and use spaces to separate words from the text\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\" \").alias(\"word\"))\n    tmp = tmp.explode('word')\n    # 计算每一个的单词长度\n    # Calculate the length of each word\n    tmp = tmp.with_columns(pl.col('word').map_elements(lambda x: len(x)).alias(\"word_len\"))\n    # 删除单词长度为0的数据\n    # Delete data with a word length of 0\n    tmp = tmp.filter(pl.col('word_len')!=0)\n\n    return tmp\n# feature_eng\ndef Word_Eng(train_tmp):\n    aggs = [\n        # 统计单词长度大于 i+1 的单词个数\n        # Count the number of words with a length greater than i+1\n        *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_{i+1}_cnt\") for i in range(15) ],\n        # 其他\n        # other\n        pl.col('word_len').max().alias(f\"word_len_max\"),\n        pl.col('word_len').mean().alias(f\"word_len_mean\"),\n        pl.col('word_len').std().alias(f\"word_len_std\"),\n        pl.col('word_len').quantile(0.25).alias(f\"word_len_q1\"),\n        pl.col('word_len').quantile(0.50).alias(f\"word_len_q2\"),\n        pl.col('word_len').quantile(0.75).alias(f\"word_len_q3\"),\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ntmp = Word_Preprocess(train_df)\n# 将新生成的特征数据和之前生成的特征数据合并\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\n\ntmp = Word_Preprocess(test_df)\ntest_feats = test_feats.merge(Word_Eng(tmp), on='essay_id', how='left')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\nmodelpath = '/kaggle/input/aes2-400-20240419134941'\ndeberta_oof = joblib.load(modelpath+'/oof.pkl')\nprint(deberta_oof.shape, train_feats.shape)\n\nfor i in range(6):\n    train_feats[f'deberta_oof_{i}'] = deberta_oof[:, i]\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\n\ntrain_feats.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Pretrain model score**","metadata":{}},{"cell_type":"code","source":"MAX_LENGTH = 1024\nMODEL_PATH = modelpath + '/*/*'\nEVAL_BATCH_SIZE = 1\nmodels = glob(MODEL_PATH)\ntokenizer = AutoTokenizer.from_pretrained(models[0])\n\ndef tokenize(sample):\n    return tokenizer(sample['full_text'], max_length=MAX_LENGTH, truncation=True)\n\nds = Dataset.from_pandas(test).map(tokenize).remove_columns(['essay_id', 'full_text'])\n\nargs = TrainingArguments(\n    \".\",\n    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n    report_to=\"none\"\n)\n\npredictions = []\nfor model in models:\n    model = AutoModelForSequenceClassification.from_pretrained(model)\n    trainer = Trainer(\n        model=model,\n        args=args,\n        data_collator=DataCollatorWithPadding(tokenizer),\n        tokenizer=tokenizer\n    )\n    preds = trainer.predict(ds).predictions\n    predictions.append(softmax(preds, axis=-1))\n    del model, trainer\n    torch.cuda.empty_cache()\n    gc.collect()\n\npredicted_score = 0.\nfor p in predictions:\n    predicted_score += p\n\npredicted_score /= len(predictions)\n\n# Meta Features(deberta-v3-large)\nfor i in range(6):\n    test_feats[f'deberta_oof_{i}'] = predicted_score[:, i]\n\n# Features number\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], test_feats.columns))\nprint('Features number: ',len(feature_names))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df = pd.concat((merged_df, train_feats), axis=1)\n\nmerged_df_test = pd.concat((merged_df_test, test_feats), axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **LGBM**","metadata":{}},{"cell_type":"code","source":"merged_df = merged_df.drop(['essay_id','score'], axis=1)\nmerged_df_test = merged_df_test.drop(['essay_id'], axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def quadratic_weighted_kappa(y_true, y_pred):\n    y_true = y_true + a\n    y_pred = (y_pred + a).clip(1, 6).round()\n    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    return 'QWK', qwk, True\n\n\n# metric and objective based on public notebooks\n\ndef qwk_obj(y_true, y_pred):\n    labels = y_true + a\n    preds = y_pred + a\n    preds = preds.clip(1, 6)\n    f = 1/2*np.sum((preds-labels)**2)\n    g = 1/2*np.sum((preds-a)**2+b)\n    df = preds - labels\n    dg = preds - a\n    grad = (df/g - f*dg/g**2)*len(labels)\n    hess = np.ones(len(labels))\n    return grad, hess\na = 2.998\nb = 1.092\n\n\n\nskf = StratifiedKFold(n_splits=15, shuffle=True, random_state=42)\n\nscores = []\n\ntrain['oof'] = 0\n\ntest_preds = []\n\nfor fold, (train_idx, valid_idx) in enumerate(skf.split(train['full_text'], train['score'])):\n    print(f\"Fold: {fold}\")\n    print(f\"Train size: {len(train_idx)}\")\n    print(f\"Valid size: {len(valid_idx)}\")\n    print()\n\n\n    X_train = merged_df.iloc[train_idx].values\n    X_valid = merged_df.iloc[valid_idx].values\n\n\n    y_train = train['score'].astype('float32').values[train_idx]\n    y_valid = train['score'].astype('float32').values[valid_idx]\n\n\n    y_train = y_train -a\n    y_valid = y_valid -a\n\n\n\n    model = lgb.LGBMRegressor(\n                objective = qwk_obj,\n                metrics = 'None',\n                learning_rate = 0.01,\n                n_estimators=10000,\n                random_state=42,\n                extra_trees=True,\n                class_weight='balanced',\n                verbosity = - 1)\n\n    callbacks = [lgb.early_stopping(500, verbose=True, first_metric_only=True), lgb.log_evaluation(period=500)]\n\n\n    predictor = model.fit(X_train,\n                                  y_train,\n                                  eval_names=['train', 'valid'],\n                                  eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                                  eval_metric=quadratic_weighted_kappa,\n                                  callbacks=callbacks)\n\n    valid_preds = predictor.predict(X_valid)\n\n    train.loc[valid_idx, 'oof'] = valid_preds + a\n\n    score = quadratic_weighted_kappa(y_valid, valid_preds)\n    scores.append(score[1])\n\n    test_preds.append(predictor.predict(merged_df_test) + a)\n\n    print(f\"Train QWK: {score}\")\n\nprint(f\"Mean QWK: {np.mean(scores)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **thresholds**","metadata":{}},{"cell_type":"code","source":"optimal_thresholds = [1.5, 2.5, 3.5, 4.5, 5.5]\n\ndef classify_with_thresholds(oof_clip, optimal_thresholds):\n\n    # Класифікуємо значення oof за допомогою оптимальних порогів\n    classified_oof = np.empty_like(oof_clip)\n    for i, threshold in enumerate(optimal_thresholds):\n        if i == 0:\n            classified_oof[oof_clip < threshold] = 1\n\n        else:\n            classified_oof[(oof_clip >= optimal_thresholds[i-1]) & (oof_clip < threshold)] = i + 1\n\n        if i == 4:\n            classified_oof[oof_clip >= threshold] = 6\n\n    return classified_oof.astype(int)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimal_thresholds = [1.5, 2.5, 3.5, 4.5, 5.5]\noof_clip = train['oof'].clip(1, 6)\nclassified_oof =classify_with_thresholds(oof_clip, optimal_thresholds)\nbest_kappa_score = cohen_kappa_score(train['score'].astype(int).values, classified_oof, weights='quadratic')\n\nfor i in range(5):\n    i_threshold=optimal_thresholds[i]-0.5\n    for j in range(1000):\n        thresholds=[x for x in optimal_thresholds]\n        thresholds[i]=i_threshold+j/1000\n\n        classified_oof = classify_with_thresholds(oof_clip, thresholds)\n        threshold_kappa_score=cohen_kappa_score(train['score'].astype(int).values, classified_oof, weights='quadratic')\n\n        if threshold_kappa_score>=best_kappa_score:\n            best_kappa_score=threshold_kappa_score\n            optimal_thresholds[i]=thresholds[i]\n\noptimal_thresholds","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_preds = (np.mean(test_preds, axis=0))\nlight_gbm_preds = np.clip(final_preds, 1, 6)\nlight_gbm_preds = classify_with_thresholds(light_gbm_preds, optimal_thresholds)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **output**","metadata":{}},{"cell_type":"code","source":"sample_submission['score'] = light_gbm_preds\nsample_submission.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}